[{"body":"","link":"https://ryji.github.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"引言 orleans 项目中，考虑因果性时钟的一种实现。\n实现 实现要点  能够保持输出时间的递增性 保证不会因为服务器与NTP服务器同步时间，导致时间戳倒退  代码 1public class CausalClock 2{ 3private readonly object lockable = new object(); 4private readonly IClock clock; 5private long previous; 67public CausalClock(IClock clock) 8{ 9this.clock = clock ?? throw new ArgumentNullException(nameof(clock)); 10} 1112public DateTime UtcNow() 13{ 14lock (this.lockable) 15{ 16// 取最近一次tick + 1 与当前时间的tick的最大值 17var ticks = previous = Math.Max(previous + 1, this.clock.UtcNow().Ticks); 18return new DateTime(ticks, DateTimeKind.Utc); 19} 20} 2122public DateTime Merge(DateTime timestamp) 23{ 24lock (this.lockable) 25{ 26var ticks = previous = Math.Max(previous, timestamp.Ticks); 27return new DateTime(ticks, DateTimeKind.Utc); 28} 29} 3031public DateTime MergeUtcNow(DateTime timestamp) 32{ 33lock (this.lockable) 34{ 35var ticks = previous = Math.Max(Math.Max(previous + 1, timestamp.Ticks + 1), this.clock.UtcNow().Ticks); 36return new DateTime(ticks, DateTimeKind.Utc); 37} 38} 39} ","link":"https://ryji.github.io/post/csharp/orleans/transaction/orleans_transaction_causalclock/","section":"post","tags":["csharp","code analyse"],"title":"Causal Clock "},{"body":"","link":"https://ryji.github.io/","section":"","tags":null,"title":"Clarity"},{"body":"","link":"https://ryji.github.io/tags/code-analyse/","section":"tags","tags":null,"title":"code analyse"},{"body":"","link":"https://ryji.github.io/tags/csharp/","section":"tags","tags":null,"title":"csharp"},{"body":"","link":"https://ryji.github.io/categories/orleans/","section":"categories","tags":null,"title":"orleans"},{"body":"","link":"https://ryji.github.io/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://ryji.github.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://ryji.github.io/categories/transaction/","section":"categories","tags":null,"title":"transaction"},{"body":"","link":"https://ryji.github.io/tags/design/","section":"tags","tags":null,"title":"design"},{"body":"","link":"https://ryji.github.io/tags/pattern/","section":"tags","tags":null,"title":"pattern"},{"body":"","link":"https://ryji.github.io/categories/release-it/","section":"categories","tags":null,"title":"Release It"},{"body":"本文为 Release It! 第五章笔记的第二部分，主要介绍服务/应用设计中可以提高系统可用性的模式 (patterns)。这些模式可以降低、减缓或者消除因局部系统出错导致全部服务出错的可能。\nSteady State (稳态) 运维/开发人员对生产/测试/开发环境中服务器配置的修改会导致系统处于不稳定的状态。通过流程自动化，在减少/消除人为干预的前提下，完成产品的发布，避免人员错误导致的系统风险。\n随着系统的运行，数据库中的记录条数、缓存对内存的占用、日志数量都会不断增多。Steady State 指的是，必须有对应的手段减少这些随着系统运行不断累计增多的资源，将系统保持在一个比较稳定的状态。\n数据清洗 随着系统的运行，数据库中的数据会逐渐增长，数据库服务器 IO 占用不断增高导致系统变慢。除了分库分表外，还可以将数据分为热数据和冷数据，两者采用不同的存储方案。\n开发人员需要保持热数据所在的数据库中的记录数量稳定，且数据清洗过程不会导致应用出错。\n日志 如果日志需要保存在服务器(第三方组件、遗留代码)上，那么最好使用类似 Kafka 的循环日志记法(限制日志的保存时间或磁盘占用空间，使用 logrotate 工具可以很方便地实现这个功能)。\n新的应用，特别是使用容器化部署方式的应用，最好采用集中化的日志存储方案。所有服务通过 Logstash 等组件将日志集中在同一个地方，建立索引后供开发/运维人员检索。\n内存中的cache 建立缓存时，需要考虑两个问题：1.key 的数量是否有上限；2.缓存的数据是否会被更改。如果缓存的 item 数量无上限，那么必须限制缓存的数量或者缓存占用的内存。如果缓存的数据需要进行修改，那么必须限制缓存的生效时间，或者采用定时刷新缓存的方案。\n总结\n 避免/减少人为干预 结合业务逻辑进行数据清洗 清理日志 限制缓存对内存的占用  Handshaking ，Shed Load，Create Back Pressure 服务器应能够保护自己，限制(throttling)自身承载的工作负载。\nHandshaking (握手) 在底层通信协议(RS232, TCP)中，handshaking 应用得比较广泛，可以让接收端通知发送端停止发送数据，直到接收端准备完毕。但是在 HTTP 等应用层协议中，handshaking 未被充分利用，导致 Cascading Failures。\n通过在服务中加入 health check，负载均衡组件能够获取服务器的负载状态，避免继续对过载的服务继续转发请求。更进一步，当服务发现自身无法保障 SLA 时，可以停止 health check 的响应，这样负载均衡组件就会认为服务不可用，避免服务接收到更多的请求。\nShed Load (丢弃负载) 与 handshaking 相似，shed load 描述的是将系统无法承载的请求丢掉来降低系统负载的方案。通常可以结合SLA，限制系统并发请求的数量来丢弃过多的请求。\nCreate Back Pressure (背压) 在使用异步调用机制时，如果上游生产消息的速度大于下游消费消息的速度，就会导致消息在队列中的堆积，新产生的消息迟迟无法得到处理，在外部看来，就好像是消费者已经挂掉一样。\n在这种情况下，一方面可以对队列中的消息进行监控，及时发现此类问题；另一方面，由于消费者的消费能力有限，可以通过背压调节生产者的生产速度，阻止生产者过快地生产消息。\n背压的调节机制往往会导致线程阻塞。因此，系统一般使用一个线程池接受请求，另一个线程池处理请求。当处理请求的线程因背压而阻塞时，接受请求的线程会因超时而返回错误。\n在系统的边界处，如果使用背压，调用方的线程可能会阻塞，导致调用方挂掉。因此，在系统边界处，一般使用 shed load 模式或者异步调用机制来保护服务的调用方。\n总结\n 建立协作式的请求/响应控制 使用 health check 使用负载均衡获取服务状态 自定义的底层协议增加 Handshaking 机制 在系统内部使用背压传递压力，跨越系统边界时使用 shed load  Test Harnesses (测试套件) 因为负载和网络环境的差异，QA 环境中往往无法提前发现生产环境的问题。大部分 QA 只是在系统的所有依赖项运行良好的情况下，测试功能是否正常。更好的手段是，通过导致系统失败的模式来进行测试。因此，需要使用测试套件来模拟远端系统，建立异常列表，对系统的质量进行测试。\n因对远端的请求是网络请求，其 socket 连接可能会受如下因素导致失败：\n 连接被拒绝 连接被放入 listen queue 中，直到 timeout 远端发送 SYN/ACK 之后，不发送数据 远端只发送 RESET 远端声明 receive window 已满，无法再发送数据， 连接建立后，始终不发送数据 连接建立后，存在丢包导致重传 连接建立后，远端永远不返回ACK，导致无限重传 HTTP 服务端接收请求后，返回 http header 之后，不返回 http body 服务返回数据非常慢(1 byte per 30 seconds) 服务返回的数据类型与约定不一致(HTML VS XML) 服务返回的数据大小与预期不符 (MB VS KB) 服务拒绝所有的证书  测试套件可以在服务中注入诸如此类的错误，进一步发掘服务之间的依赖项。混沌工程(chaos engineering)是使用此模式的一种方案。\n总结\n 模拟未在文档中声明的错误 给服务的调用方增加压力 test harness 是其它测试方法的补充，无法取代其它测试方法的作用  Decoupling Middleware (中间件解耦) RPC、HTTP都属于同步调用方案(sync call-response, request-reply)，其优点为逻辑简单，缺点为两个系统之间紧耦合，两个系统必须同时处于可用状态才能请求成功。\n基于队列的 MQ 或者 publish/subscribe 的消息组件时一种松耦合方案，它不要求两个系统同时处于可用状态，可以防止系统出现 Cascading Failures。松耦合的方案在请求方需要获取服务提供方的返回值时，往往需要更复杂的设计方案。\n总结\n 尽早决定是否采用中间件进行解耦 完全解耦能够避免很多导致系统失败的模式 学习更多的架构方案  Governor (调节器) 流程自动化/控制面出现错误时，对整个系统的破坏力是巨大的。因此，对某些有风险的自动操作(关闭节点，删除数据，block ip)，需要降低控制面的处理速度，或者引入人工确认环节，降低风险。\n总结\n 增加流程的阻力，降低速度，允许人为介入 在有风险的操作中，引入人工确认 建立风险曲线，短时间的大量低风险的操作也可能引入高风险(短时间大量创建节点)  ","link":"https://ryji.github.io/post/release_it/stability_pattern_part2/","section":"post","tags":["stability","design","pattern"],"title":"Release It 读书笔记 - Stability patterns (part2)"},{"body":"","link":"https://ryji.github.io/tags/stability/","section":"tags","tags":null,"title":"stability"},{"body":"","link":"https://ryji.github.io/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/","section":"categories","tags":null,"title":"读书笔记"},{"body":"本文为 Release It! 第五章笔记的第一部分，主要介绍服务/应用设计中可以提高系统可用性的模式 (patterns)。这些模式可以降低、减缓或者消除因局部系统出错导致全部服务出错的可能。\nTimeout (超时时间) 在进行网络请求时，设置超时时间是很常见的。然而，在系统集成时，第三方的 API 往往不会提供超时时间参数供使用者调用。这时需要采用 timeout 模式调用这些 API，避免因第三方系统的问题导致服务崩溃。\n另外，即使在一个系统内部，也会有需要设置超时时间的场景。例如，访问资源池时，访问线程可能会被 block，这时，如果采用 timeout 模式，这些被 block 的线程会在达到超时时间后返回，防止大多数线程被同一个资源阻塞，导致系统无线程可用。另外，使用同步或者信号量原语时，最好也增加 timeout 参数，避免无限制的等待。\n另外，在过去的经验中，超时往往会和重试在一起使用。在使用重试时，除了瞬态错误外(丢包)，大部分错误(配置出错，服务出错)都无法通过重试恢复。在 GUI 编程中，因进行重试而阻塞 UI 界面会降低用户体验。在部分场景中，将任务排入队列，在稍晚的时间进行重试也可能会提高系统的鲁棒性。总之，在使用重试时，需要仔细考虑业务要求和错误类型。\ntimeout 模式和 Fail Fast 模式一体两面，前者用于处理 outbound requests，后者用来处理 incoming requests。\n总结\n 在系统集成点、阻塞线程和资源池访问时使用 timeout 使用 timeout 从异常中恢复 考虑延迟后进行重试  Circuit Breaker (断路保护器) 断路保护器来源于电气学科，共包含三种状态：Closed、Open、Half-Open。断路保护器会代理被保护的请求，并通过统计请求成功/失败的频次来调整自身的状态。如图所示。\n     状态为 Closed 时，所有的请求都会被转发到实际的服务中，如果请求成功，返回请求结果；如果请求失败，失败频次累加，当失败频次达到上限，则断路保护器会转移到 Open 状态。 当保护器状态为 Open 时，所有的请求都立即返回失败。状态为 Open 的断路保护器经过一定的超时时间后，状态转移到 Half-Open。 状态为 Half-Open 的断路保护器会尝试转发一部分请求(一般为一个)到实际的服务中，如果成功，断路保护器转移到 Closed，失败则转移到 Open。  断路保护器也用于处理 outbound requests，它提供了一种自动降级的机制：当系统需要的资源不可用时，不再对其进行频繁访问。在断路保护器状态为 Open 时，往往需要使用 fallback 进行处理。\n对于系统运维来说，断路保护器的状态非常重要。因此，使用此模式时，需要将其状态转移记录日志，将其状态(Closed、Open、Half-Open)暴露到运维终端。另外，还需要提供一种在外部控制保护器状态的方法，运维人员可根据系统运行状态手工控制保护器的开关，避免保护器参数设置不合理导致的系统崩溃。\n总结\n 如果系统不可用，停止调用此系统 和 timeout 配合使用 暴露、追踪、报告断路保护器的状态  Bulkheads (隔离仓) bulkhead 来源于海运，原为当船只进水时，将船分割成多个小单元，防止水淹没整只船的隔舱。在设计软件系统时，也可以应用相似的思路，将失败的子系统与正常子系统隔离，防止整个系统失败。\n从程序运行的环境来说，可用的 bulkhead 方式是物理冗余：即使部分实例失败，也不会影响其余实例的运行。除了物理机、虚拟机的冗余外，当使用FAAS(functions as a service)时，实际上每次调用都在不同的环境中运行，这也是一种 bulkhead 的方式。\n从服务编排的角度来说，将公共服务的实例划分为不同的实例池，供不同的服务调用时另一种实现 bulkhead 的方式。这样，即使某一个服务过载，导致其对应的公共服务池崩溃，也不会影响其它服务和他们对应的公共服务池的使用。\n从(容器)来说，容器初始化时，往往制定其使用的CPU/memory上限。即使某一容器发生内存泄漏，也不会将整个系统的可用内存耗尽，不会影响其它容器的创建和运行。\n从服务内部来说，bulkhead 是将线程池等资源划分，不同的服务/业务使用不同的线程池。这样，即使某一业务线程资源耗尽，也不会影响其它部分接受请求。这样也可以帮助服务保留供 admin 用户使用的资源(管理服务状态、日志、trace 等)\n总结\n 保护整个系统 选择合理的隔离尺度(应用中的线程池、服务器上的CPU、集群中的服务) 使用 SOA 或者微服务架构时，考虑bulkhead  Fail Fast (快速失败) 如果系统可以预先确定操作会失败，那么可以使用 fail fast 模式来防止系统进行一段时间的处理后再返回错误消息。\n负载均衡组件或者服务网关组件可以在服务不可用时，直接返回错误消息。错误的配置会导致请求不会立刻返回，而是在这些组件中等待一段时间，但是等待期间，目标服务往往无法恢复。\n当应用处于繁忙状态无法再处理新的请求时，可以立即返回失败消息。\n另外，对用户的输入进行检查，也是 Fail Fast 模式的一种实现。用户的输入不合法或者用户的客户端资源不足(磁盘/内存)时，可以立即返回异常信息，让用户进行处理。\n使用 Fail Fast 模式时，要注意区分资源耗尽和用户输入的异常，防止用户输入不合法触发上游的断路保护器。\n总结\n 避免慢响应 预先检查资源占用情况 校验用户输入  Let It Crash (让它崩溃) 当发生无法/很难恢复的错误时，让出现错误的组件崩溃往往是唯一的选择。为了能够应用 let it crash 模式，需要满足如下条件：\n限制粒度 崩溃部分必须有可控的边界。其它与之有关联的系统必须能够保护自身，不会因崩溃部分而崩溃。\n快速替换 实例崩溃后，必须能够快速创建其替代品。\n当使用容器时，整个容器的启动时间往往在毫秒到秒级别，此时可以直接让容器 crash 并创建新的容器。\n当使用虚拟机时且进程启动时间为秒级别时，由于虚拟机的启动时间往往在分钟级别，此时最好是让虚拟机上运行的进程 crash。\n当服务进程的启动时间是分钟级别时，最好不要使用 let it crash 模式，而应该尽量从错误中恢复。\n监督管理 当错误出现时，该组件的管理单元可以让该组件崩溃并重启一个新的组件。如果错误持续频繁出现，此时可能是因为管理单元内部的状态出现了问题，最好将此管理单元 crash，并重新创建此管理单元。监督管理单元按层级结构划分，直至到进程的根节点。\n需要注意的是，autoscaling 组件和这里的监督管理组件有所不同。autoscaling 总是会重启崩溃的实例，而且autoscaling 组件没有方向性，也没有层次结构。\n再整合 let it crash 实施的必要条件时，系统的其余部分能够在创建新的崩溃组件实例后获得通知，并且能够使用此组件。如果崩溃的是一个服务，那么在此启动的服务需要能够注册到服务发现组件中，这样负载均衡组件才能将请求分配过来。\n总结\n 为了保护系统，可以让组件崩溃 快速启动并整合资源 隔离崩溃的组件 不要让单体应用崩溃  ","link":"https://ryji.github.io/post/release_it/stability_pattern_part1/","section":"post","tags":["stability","design","pattern"],"title":"Release It 读书笔记 - Stability patterns (part1)"},{"body":"","link":"https://ryji.github.io/tags/kafka/","section":"tags","tags":null,"title":"kafka"},{"body":"Apache Kafka 是一种 publish/subscibe 类型的消息系统，常用作系统间异步通信的中间件，降低系统的耦合。其突出优点为：支持多生产/消费者、基于磁盘的消息存储、水平扩展性、高性能。\nmessage \u0026amp; batch kafka 中的基本数据单元是消息(message)，其概念类似于数据库中的行。为了提高读写效率，消息是以批(batch)的形式写入 kafka 中，一个 batch 是一组消息的集合。通过这种方法，降低传输消息的网络损耗；另外，batch 一般经过压缩，进一步提高了数据传输效率。batch 的大小和消息传输的时延(latency)正相关。\n在 kafka 中，虽然消息只是一组byte，但通常推荐将消息组织成某种可序列化/反序列化的格式。常用的格式有 JSON, XML 和 Avro 。\ntopic 在 kafka 中，消息使用主题(Topic)分类，其概念类似于数据库中的表。不同类型的消息使用不同的 topic，应用系统只需发布/订阅自身业务相关的 topic 消息。每一个 Topic 又有多个分区(partition)。在一个 kafka 集群中，不同的分区可以分布在不同的节点上，提高消息系统的水平扩展能力；同一个分区可以在多个节点上冗余，提高消息系统的容灾和可用性。\nproducer \u0026amp; consumer Producer(生产者)是 kafka 中产出消息的单元。在默认情况下，生产者不关心消息发送到哪个分区，系统会自动进行消息的负载均衡。\nConsumer(消费者)是订阅并消费消息的单元；一组订阅同一个 topic 的 consumer 构成了一个 consumer group。在一个 consumer group 中， 同一个 partition 的消息最多只能被一个 consumer 消费，即对任一一个topic：consumer 的数量 \u0026lt;= partition 数量。某些极其热门的消息应分配更多的分区，防止 partition 的数量造成整个系统的性能瓶颈。\nkafka 中，每条消息都会有其对应在当前 partition 中的 offset，消费者通过消息的在该 partition 的偏移量(offset)来进行消费。实际中，可以在 ZooKeeper 或者 kafka 中记录当前topic - partition 被当前的 consumer group 消费的 offset，可以保障 consumer 重启后也不会丢失其处理消息的位置。\nretention \u0026amp; compacted kafka 提供了retention(消息保留)和 log compacted(消息压缩)功能，其中消息保留可以按照消息的时间或者 topic 的占用磁盘的大小进行配置；消息压缩功能对同一个key的数据，只保留最后一条记录，实际生产中，这一特性主要用于changelog类型的数据。\nbroker \u0026amp; cluster 单一的 kafka 服务叫做 broker，一组 broker 构成一个 cluster。集群提高了kafka系统的性能和可用性。在多数据中心环境中，往往需要多个 kafka cluster 进行数据同步。kafka提供了叫做 MirrorMaker 的工具完成此任务。MirrorMaker 其实就是同时是 kafka 的生产者和消费者，消费一个集群的消息，并将这些消息生产到另一个集群。\n","link":"https://ryji.github.io/post/kafka/kafka_basic_intro/","section":"post","tags":["kafka"],"title":"Kafka basic intro"},{"body":"","link":"https://ryji.github.io/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","section":"categories","tags":null,"title":"消息队列"},{"body":"","link":"https://ryji.github.io/tags/antipattern/","section":"tags","tags":null,"title":"antipattern"},{"body":"本文为 Release It! 第四章的笔记的第二部分，主要介绍服务/应用设计中可能引入的反模式 (antipatterns)，这些错误的设计会导致、加速系统的崩溃。在系统设计时，应尽可能地避免这些反模式。然而，错误是不可避免的，因此，还需要进一步了解这些错误发生后，会如何影响整个系统的运行。\nBlocked Threads (阻塞的线程) 在访问处理临界区的资源(cache, resource pool, 外部系统)时，线程会被其他同时访问此资源的线程阻塞。线程阻塞往往是随机出现的，出现的概率随着系统负载的增大而增大。在开发环境，由于系统的用户远低于生产环境的用户，很难发现此类问题。\n在领域模型中使用线程同步方法是一种反模式，这种设计使得应用无法横向扩展。一种避免使用线程同步方法的设计是使领域模型对象不可变(immutable)，使用此对象进行查询和展示；当需要改变状态时，需要构造 Command 对象，也就是 CQRS(Command Query Responsibility Separation) 模式。\n总结\n 阻塞的线程是大多数错误产生的最直接原因 仔细检查资源池 使用经过验证的多线程原语 使用 Timeouts 进行防御 留意引用的第三方库  Self-Denial Attacks (自我拒绝攻击) Self-Denial Attacks是一种人和系统共同参与下发生的巧合，其结果类似于外部对系统进行攻击。\n例如，超级话题，电商平台中 bug 优惠券、bug 价，都会使得系统在短期内产生对同一对象的大量请求。这些请求有可能会拖垮整个系统，导致失败。\n处理这种问题的理想手段是使用 share-nothing 架构。然而，系统间总是需要共享部分数据。因此，在必须要共享资源时，尽可能地使用中间件进行解耦；在全局锁失效时，使用 fallback 方案进行处理(悲观锁-\u0026gt;乐观锁)\n总结\n 保持沟通(商务事件) 保护共享资源 对热点数据快速重分配(热点数据自动多点备份，防止单机访问瓶颈)  Scaling Effects (扩展的影响) 随着系统的横向扩展，一些在开发/测试环境中不存在的问题会逐渐在生产环境中暴露出来。\n在非生产环境或者传统架构中，P2P 通信是一种常见的策略。但随着系统进行横向扩展，这种通信方式会导致O($N^2$)的数据流量，因此，推荐采用以下几种方式进行通信：\n UDP 广播(网络效率不高) TCP/UDP 组播 publish/subscribe 模式 消息队列  共享资源/服务，随着系统 scaling，会慢慢变成系统的瓶颈。需要尽可能减少资源的共享，做到share-nothing。\n总结\n 检查测试环境和生产环境的 scale 规模，确定是否会有 scaling effect 注意 P2P 通信 注意共享资源  Unbalanced Capacities (容量不平衡) 在生产环境中，访问前端服务的客户端(浏览器/App)可能数以百万计，在这种情况下，前端服务和后端服务、后端服务和后端服务之间都可能会发生负载不平衡的问题。随着系统的运行，热门和非热门的服务时刻变化，这进一步导致了不同系统之间的容量不平衡。热点事件的发生，也会加剧这种问题。\n总结\n 检查前后端服务器和线程数量的比例 注意 scaling 和用户对系统的影响 测试环境虚拟化，并对测试环境按生产环境等比例 scale up 前后端自测，检查系统在高负载/调用的系统停止响应时能否正常工作  Dogpile (瞬态负载) 当一组服务同时暴露在瞬态的高负载下时，这种现象叫做 dogpile。它在如下情况下发生\n 同时启动多个服务实例(代码更新、重启) 定时任务 配置信息变更后，配置系统推送配置变更消息（一些配置系统允许配置随机延迟时间来降低配置变更后配置系统被其他系统访问的并发量） 多线程被同一个信号量阻塞，当信号量恢复时，下游系统会产生较大的瞬态负载 大量缓存同时失效，对数据库系统的负载  总结\n dogpile 导致系统必须处理不必要的高负载 使用随机的延时时间来分散请求 使用退避算法来避免脉冲式的负载压力  Force Multiplier (影响倍增) 随着系统运维自动化程序的提升，对系统进行变更造成的影响也随之倍增。特别的，在程序的控制面(control plane, 基础设施和应用元信息管理)产生的错误(数据不一致、数据损坏、误操作等)，往往会造成重大影响。\n在这些控制面相关的系统中，可以采用如下的安全措施：\n 如果发现大于 80% 的系统不可用，大概率是因为控制面系统不可用，而不是应用程序不可用 对危险操作进行滞后处理：快速启动新的节点(在公有云上，此举可能会产生巨额账单)，慢慢关闭不可用的节点 当期望的状态和实际状态差别过大时，需要人工确认后再进行自动操作 消耗资源的操作需要是有状态的，避免因消耗资源/资源检查时间不匹配导致问题(监测 container 的系统，在启动一个 container 用时 10 min，1s 进行一次 container 数量检查时，为了创建一个 container，可能会误创建 $10*60=600$ 个container) 创建统计资源缓冲区，假定系统过载检查 1s 一次，在需要 5min 来创建VM时，确保不会创建 300 个新的VM  总结\n 在修改生产环境前寻求帮助 注意创建资源滞后时间 和 资源统计间隔时间 不匹配的影响 注意控制系统的不可靠性  Slow Responses (慢响应) 导致系统慢响应的原因通常是系统接受了过多的请求。当所有的资源都用于忙碌状态时，无法创建新的资源来处理新的请求。另外，由于内存泄漏导致系统不停进行GC也会导致慢响应；网络拥塞、Socket buffer 区被撑爆也会导致系统慢响应。\n系统应有能力判断自身的响应时间是否达到SLA，当无法达到时，对新的请求，应及时拒绝(应用层/传输层均可实现)。当然，因无法达到SLA的拒绝应放在接口文档中，并征得系统调用方的同意。\n总结\n 慢响应导致级联失败 对于网站来说，慢响应导致更多的网络压力(用户点击刷新按钮) 考虑快速失败 (Fail Fast) 注意内存泄漏和资源竞争导致的慢响应  Unbounded Result Sets (无界结果集) 在访问其他系统(数据库，数据查询接口)时，需要显式限定返回数据的大小，防止因编程bug、约定修改等原因获得远超预期的数据，最终在对数据进行处理时内存耗尽导致程序崩溃。\n总结\n 使用真实环境的数据集 前端分页 不要依赖数据提供方分页 在应用协议中限定返回数据的大小  ","link":"https://ryji.github.io/post/release_it/stability_antipattern_part2/","section":"post","tags":["stability","design","antipattern"],"title":"Release It 读书笔记- stability antipatterns (part2)"},{"body":"本文为 Release It! 第四章的笔记，主要介绍服务/应用设计中可能引入的反模式 (antipatterns)，这些错误的设计会导致、加速系统的崩溃。在系统设计时，应尽可能地避免这些反模式。然而，错误是不可避免的，因此，还需要进一步了解这些错误发生后，会如何影响整个系统的运行。\nIntegration Points (系统集成点) 系统之间两两集成 (复杂度 $N^2$) vs 通过中间件集成 (复杂度 $2N$)\nTCP TCP协议具有以下特点：\n 客户端发送SYN包时，如果没有服务在监听此端口，则客户端会理解获得一个异常，此处耗时 \u0026lt; 10ms 客户端发送SYN包后，如果服务端处于过载状态，此连接可能会进入网络栈listen queue中。listen queue中的连接状态为SYN包已发送成功，但SYN/ACK未返回。如果listen queue已满，客户端发送SYN包会立即返回失败；但是，listen queue中的连接可能会被阻塞分钟级别的超时时间，导致open方法无法返回，直到服务端能够处理此连接 同样的，客户端发送消息并等待服务端的返回时，可能也会因服务端处理能力的问题而被阻塞。read方法的阻塞时间默认为最大值 TCP连接关闭后，默认TIME_WAIT时间段内不可重用此端口，避免因delay的消息发送到此端口，导致旧连接的消息发送的新创建的连接上  因此，网络栈的失败可以分为两类：快速失败和慢速失败。后者可能会导致线程被阻塞数以分钟计的时间，导致资源的消耗，降低系统的承载能力。\n理论上，socket连接是抽象的概念，只要通信的两端认为自身的连接处于open状态，即使物理连接短暂断开，在恢复后也不会影响双方的通信。但是，由于防火墙/NAT等网络中间件的存在，非活跃状态的socket连接可能会被清理出路由表，随后通信的一端发出的数据被防火墙/中间件丢弃，导致双方通信失败。此问题在数据库连接池中是一种常见的问题，通常的解决方案是：将连接池实现为FIFO的，定时使用特定SQL检测处于IDLE状态的连接。\nHTTP HTTP 协议是建立在 TCP 的基础上，因此，上一节描述的 TCP 的问题在 HTTP 协议中都有可能发生。另外，HTTP还有自身特有的一些问题：\n 服务端只接收 TCP，不接收 HTTP 请求 服务端只接受连接，不读取请求的数据。如果请求的 body 较大，可能会导致服务端的 TCP RWND 被撑爆，甚至导致请求无法发送成功 服务端可能返回请求端无法识别的状态码 服务端返回的数据类型可能与请求端期望得到的不一致，或者请求端不知道怎么处理此返回值 服务端声明的数据类型与其包含的数据不一致，如声明为 json，实际包含的为 plaintext  第三方(供应商)API库 第三方 API 库的质量往往不如服务端的质量。可能会导致阻塞的问题主要有以下几个：资源池，socket 连接，HTTP 连接等。在第三方库中发现的缺陷，往往只能通过固定时间重启应用，或者绕开异常代码来规避\n总结\n 注意不可避免的集成点 为多种类型的 fail 做准备 了解何时需要抽象，何时需要了解抽象背后的概念 错误会在系统间快速传播 应用 Circuit Breaker, TimeoutsDecoupling Middleware, 和 Handshaking 来处理此节的问题  Chain Reactions (链式反应) 横向扩展+负载均衡是提高系统可用性的常用方法。然而，由于 Chain Reactions 的存在，当一个节点因过载导致崩溃时，此节点的处理压力会转移到其它节点，导致其它节点处理压力加大，提高了其它节点崩溃的风险。在极端情况下，节点会一个个地崩溃，且崩溃之间的时间间隔逐渐降低，最终导致整个集群不可用。\nChain Reactions 的发生往往与内存泄漏、负载相关的崩溃、线程阻塞相关。\n总结\n 节点的崩溃危及系统的其他部分 查找内存/资源泄漏相关的问题 查找资源竞争/死锁问题 使用 AutoScaling 使用 Bulkhead 模式  Cascading Failures (级联失败) 级联失败指组件的失败导致调用此组件的上一次组件失败，最终导致与此组件关联的整个系统不可用。\n级联失败的一个例子是数据库失败。当数据库相应变慢时，与之有交互的服务层会产生大量等待数据库服务响应的线程，最终耗尽服务端的资源，导致服务层失败。\nCascading Failures 的原因一般是资源池(连接、线程、内存)因下一层的失败而耗尽，不限制超时时间的系统间集成往往会导致此问题。另外，在层间传播失败的机制往往与阻塞的线程相关(例如服务层访问数据库的线程被阻塞)。另外，随意增加重试机制也会导致 Cascading Failures (服务层访问数据库失败后，新起线程再次重试访问，正常的负载需要更多的线程来处理，最终服务层线程耗尽)。\n总结\n 阻止失败在层间的传递 仔细检查资源池 使用 Timeouts 和 Circuit Breaker  Users (用户) 用户在给系统带来利润的同时，也会给系统带来流量、内存、计算资源和安全的压力。\n 内存：使用弱引用来管理进程内的缓存，使用外部缓存系统(redis)来进行进程间的缓存共享 计算资源：一些用户会占用比其他用户更多的资源，而这部分用户往往是系统的高价值用户。为了满足这部分用户的需求，可以采用更加激进的测试策略。这样当更多普通用户转化为高价值用户时，系统也不会崩溃。这里要强调的是，即使进行激进的测试，也需要在硬件资源的成本和潜在用户的流失间取得平衡。 非法用户：因配置错误、程序 bug 而产生的非法请求，此问题可能会导致大量不必要的系统请求，造成类似于 DDOS 攻击的效果。另外，没有按照 robots.txt 约定访问的网络爬虫也属于非法用户。 恶意用户：非法侵入并破坏计算机系统的用户。  总结\n 用户会占用内存等资源 用户的行为是随机、非预期的 恶意用户是不可避免的 用户会组成团体(超话、热点事件)  ","link":"https://ryji.github.io/post/release_it/stability_antipattern_part1/","section":"post","tags":["stability","design","antipattern"],"title":"Release It 读书笔记- stability antipatterns (part1)"},{"body":"","link":"https://ryji.github.io/tags/docker/","section":"tags","tags":null,"title":"docker"},{"body":"","link":"https://ryji.github.io/tags/micro-service/","section":"tags","tags":null,"title":"micro service"},{"body":"","link":"https://ryji.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%85%A5%E9%97%A8/","section":"categories","tags":null,"title":"微服务入门"},{"body":"本文为微服务入门， 主要介绍如何使用 Spring boot 开发一个 Rest Api 服务，以及如何将此服务打包为 docker image 并运行。\nspring boot 实现 Rest Api 依赖项 依赖项主要有两个： spring-boot-starter-web 和 spring-boot-starter-actuator。前者为 spring Rest Api 必需组件，后者提供了health, beans, dump, env,metrics等接口，可以方便地获取服务内部的信息。\n1\u0026lt;parent\u0026gt; 2\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; 3\u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; 4\u0026lt;version\u0026gt;2.3.9.RELEASE\u0026lt;/version\u0026gt; 5\u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; 6\u0026lt;/parent\u0026gt; 7\u0026lt;dependencies\u0026gt; 8\u0026lt;dependency\u0026gt; 9\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; 10\u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; 11\u0026lt;/dependency\u0026gt; 12\u0026lt;dependency\u0026gt; 13\u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; 14\u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; 15\u0026lt;/dependency\u0026gt; 16\u0026lt;/dependencies\u0026gt; 入口 SpringBootApplication 注解标志此类为应用的入口点。\n1@SpringBootApplication 2public class DemoApplication { 34public static void main(String[] args) { 5SpringApplication.run(DemoApplication.class, args); 6} 7} controller RestController 注解标志此类为Rest Api，类上的 RequestMapping 注解为类中所有接口的 Rest Uri 前缀，v1 为服务的版本信息。服务的横向扩展特性导致运行环境存在多个实例；服务的快速迭代导致多版本的服务共存。因此，需要使用版本信息来区分和标志同一服务的不同版本。方法 getLicenses 上的注解标志此方法的对应的 Rest Api uri，同时标志此 Api 为 Http Get 请求。\n1@RestController 2@RequestMapping(value = \u0026#34;v1/organizations/{organizationId}/licenses\u0026#34;) 3public class LicenseController { 45@Autowired 6private LicenseService licenseService; 78@RequestMapping(value = \u0026#34;/{licenseId}\u0026#34;, method = RequestMethod.GET) 9public License getLicenses(@PathVariable(\u0026#34;organizationId\u0026#34;) String organizationId, 10@PathVariable(\u0026#34;licenseId\u0026#34;) String licenseId){ 11return License.LicenseBuilder.Builder() 12.withId(licenseId) 13.withOrganizationId(organizationId) 14.withProductName(\u0026#34;product name\u0026#34;) 15.withLicenseType(\u0026#34;license type\u0026#34;) 16.build(); 17} 18} service Service 注解会自动生成一个 bean，可被 controller 中通过 Autowired 注解获取。\n1@Service 2public class LicenseService { 34public License getLicense(String licenseId){ 5return License.LicenseBuilder 6.Builder() 7.withId(licenseId) 8.withLicenseType(UUID.randomUUID().toString()) 9.withProductName(\u0026#34;Test product name\u0026#34;) 10.withLicenseType(\u0026#34;Test License Type\u0026#34;) 11.build(); 12} 13} config actuator 默认只开放 health 和 info 的 http 端口，需要使用 http 查看其他信息时，需要配置暴露的端口。如下配置暴露 actuator 的所有服务端口。\n1management:2endpoints:3web:4exposure:5include:\u0026#34;*\u0026#34;docker Dockerfile \u0026amp; run Dockerfile 描述如何对 docker image 进行打包，并调用 run.sh 启动服务。一般约定这两个文件位于 docker 文件夹中，此文件夹位于 src/main 中。\n openjdk:8-jdk-alpine 是基础镜像包 在此包的基础上，添加 netcat 后，可以在 run.sh 使用 nc 命令判断服务依赖的服务是否启动，只有依赖项启动后，才会执行后续的步骤。 为了提高提高 apk 命令的执行速度，添加阿里云镜像。 拷贝 jar 包到指定目录后，执行 run.sh 脚本。  1FROMopenjdk:8-jdk-alpine2RUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\u0026#39; /etc/apk/repositories3RUN apk update \u0026amp;\u0026amp; apk upgrade \u0026amp;\u0026amp; apk add netcat-openbsd4RUN mkdir -p /usr/local/services5ADD @project.build.finalName@.jar /usr/local/services/6ADD run.sh run.sh7RUN chmod +x run.sh8CMD ./run.shrun.sh 脚本执行实际的服务启动，可以加入部分调试和等待依赖的服务启动的命令。\n1#!/bin/sh 2echo \u0026#34;********************************************************\u0026#34; 3echo \u0026#34;Starting Server\u0026#34;/@project.build.finalName@ 4echo \u0026#34;********************************************************\u0026#34; 5# while ! `nc -z database $DATABASESERVER_PORT`; do sleep 3; done 6java -jar /usr/local/services/@project.build.finalName@.jar plugins maven 提供了帮助 docker 打包的 plugin：docker-maven-plugin。在 pom 文件中添加如下：\n1\u0026lt;plugin\u0026gt; 2\u0026lt;groupId\u0026gt;com.spotify\u0026lt;/groupId\u0026gt; 3\u0026lt;artifactId\u0026gt;docker-maven-plugin\u0026lt;/artifactId\u0026gt; 4\u0026lt;version\u0026gt;0.4.10\u0026lt;/version\u0026gt; 5\u0026lt;configuration\u0026gt; 6\u0026lt;imageName\u0026gt;${docker.image.name}:${docker.image.tag}\u0026lt;/imageName\u0026gt; 7\u0026lt;dockerDirectory\u0026gt;${basedir}/target/dockerfile\u0026lt;/dockerDirectory\u0026gt; 8\u0026lt;resources\u0026gt; 9\u0026lt;resource\u0026gt; 10\u0026lt;targetPath\u0026gt;/\u0026lt;/targetPath\u0026gt; 11\u0026lt;directory\u0026gt;${project.build.directory}\u0026lt;/directory\u0026gt; 12\u0026lt;include\u0026gt;${project.build.finalName}.jar\u0026lt;/include\u0026gt; 13\u0026lt;/resource\u0026gt; 14\u0026lt;/resources\u0026gt; 15\u0026lt;/configuration\u0026gt; 16\u0026lt;/plugin\u0026gt; 其中 docker.image.name 和 docker.image.tag 可根据实际的服务名称和版本定义\n1\u0026lt;properties\u0026gt; 2\u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; 3\u0026lt;docker.image.name\u0026gt;com.example.demo\u0026lt;/docker.image.name\u0026gt; 4\u0026lt;docker.image.tag\u0026gt;v0.1\u0026lt;/docker.image.tag\u0026gt; 5\u0026lt;/properties\u0026gt; 另外，docker-maven-plugin 中指定 dockerDirectory 为 ${basedir}/target/dockerfile，因此，需要使用 maven-resources-plugin 将 src\\main\\dockerfile 拷贝到 ${basedir}/target/dockerfile\n1\u0026lt;plugin\u0026gt; 2\u0026lt;artifactId\u0026gt;maven-resources-plugin\u0026lt;/artifactId\u0026gt; 3\u0026lt;executions\u0026gt; 4\u0026lt;execution\u0026gt; 5\u0026lt;id\u0026gt;copy-resources\u0026lt;/id\u0026gt; 6\u0026lt;phase\u0026gt;validate\u0026lt;/phase\u0026gt; 7\u0026lt;goals\u0026gt; 8\u0026lt;goal\u0026gt;copy-resources\u0026lt;/goal\u0026gt; 9\u0026lt;/goals\u0026gt; 10\u0026lt;configuration\u0026gt; 11\u0026lt;outputDirectory\u0026gt;${basedir}/target/dockerfile\u0026lt;/outputDirectory\u0026gt; 12\u0026lt;resources\u0026gt; 13\u0026lt;resource\u0026gt; 14\u0026lt;directory\u0026gt;src/main/docker\u0026lt;/directory\u0026gt; 15\u0026lt;filtering\u0026gt;true\u0026lt;/filtering\u0026gt; 16\u0026lt;/resource\u0026gt; 17\u0026lt;/resources\u0026gt; 18\u0026lt;/configuration\u0026gt; 19\u0026lt;/execution\u0026gt; 20\u0026lt;/executions\u0026gt; 21\u0026lt;/plugin\u0026gt; 打包与运行 此命令在本机生成docker image\n1mvn clean package docker:build 此命令运行生存的docker image\n1docker run -it -p 8080:8080 com.example.demo:v0.1 其中，-it 交互模式运行， 8080:8080端口映射，com.example.demo:v0.1 分别为docker image name 和 tag。\n","link":"https://ryji.github.io/post/spring/spring_boot_with_docker/","section":"post","tags":["micro service","docker"],"title":"微服务入门 - spring boot Rest Api \u0026 docker"},{"body":"引言 TLS (Transport Layer Security)建立在传输层之上，为其它协议提供跨网络的安全传输能力，这种能力包含三个部分：加密、认证、防篡改和伪造。TLS通过非对称加密技术，在非加密信道中，为无先验信息的通信双方提供协商会话秘钥(对称秘钥)的方法；在TLS握手过程中，协议规定了一系列认证方法，使通信双方能够互相认证；最后，TLS提供消息校验码(message authentication code，MAC)来对每一条传输的消息进行校验，防止传输途中的篡改和伪造。\nHTTP协议的成功，催生出很多网络中间件：缓存服务器、安全网关、内容过滤器等。基于HTTP协议的新协议可能无法被这种中间件识别，或者会被这些中间件盲目地篡改。因此，在WebSocket协议中，一般会推荐使用HTTPS作为其通信的信道，这样防止消息被中间件篡改。\nTLS握手 TLS协议握手过程如下图所示：\n   时刻 事件     0 ms 建立TCP信道   56 ms 客户端发送其支持的TLS信息，包括版本，支持的加密组件列表，随机数rand_c   84 ms 服务端根据客户端发送的信息，选择一种加密组件，附带服务端的证书和随机数rand_s，发送到客户端   112 ms 如果双方对版本和加密方式达成一致，且服务端的证书在客户端认证通过。客户端生成一个对称秘钥key，使用服务端的公钥加密后的发送到服务端，并通知服务端切换到加密传输模式   140 ms 服务端使用私钥解密客户端发送的对称秘钥key，并通过消息的MAC校验消息的完整性。基于对称秘钥key, rand_c, rand_s计算出随后与客户端通信使用的秘钥，并基于此秘钥发送加密的finish消息   168 ms 客户端基于对称秘钥key, rand_c, rand_s计算出对称秘钥，使用此秘钥解密服务端的消息。至此，加密通信信道完全建立        协议扩展  ALPN（Application Layer Protocol Negotiation）  使用自定义协议时，通信的双方需要进行协商，例如http协议升级到2.0。使用ALPN技术，可以利用现有的TLS 443端口协商服务协议，其过程为：客户端在ClientHello消息中增加ProtocolNameList字段，其值为客户端支持的协议列表，服务端读取ProtocolNameList后，根据自身支持的协议，在ServerHello消息中，增加ProtocolName字段，其值为服务端选定的自定义协议。\n SNI(server Name Indication)  同一个IP地址host多个不同的服务，每个服务都有各自的TLS证书\nTLS Session重用 完整的TLS握手流程会给应用程序造成比较大的延时，因此，人们提出了一些重用已经建立的TLS会话的方法来降低因安全导致的性能损失。\nTLS Session Id 服务端可以在ServerHello消息中包含一个32字节的Session Id，如果客户端和服务端都将这个ID存储在缓存中，那么在接下来的TLS握手中，客户端可以在ClientHello消息中包含此ID，这样服务端就可以判断客户端是在重用已建立好的TLS会话，这样提高应用性能。例如，虽然浏览器会与同一个服务端建立多个连接来并行请求资源，但只有第一个连接进行了完整的TLS握手流程，其它连接会等待第一个TLS握手建立完毕后，重用其TLS Session Id\n    但这个方案会导致服务器需要创建并维护每一个客户端的TLS session缓存，对于访问量比较大的应用来说，每台服务器可能每天需要创建上百万条TLS session缓存，这会给服务器带来巨大的内存消耗。虽然可以在外部引入缓存服务器来存储这些缓存，但这也会增加运维的难度和成本。\nTLS Session Ticket Session Ticket解决了服务端必须缓存TLS sesssion的问题。在一次完整的TLS握手过程中，服务端发送给客户端一个ticket，这个ticket使用一个服务端的私钥进行加密。客户端在后续的clientHello消息中，可以在SessionTicket字段中加入此ticket。因为ticket使用服务端的私钥进行加密，因此，无法再传输过程中被篡改和伪造。相对于Session Id的方案，此方案的服务端是无状态的。\n在实践中，部署负载均衡的TLS Session Ticket服务也需要进行谨慎的规划，所有隐藏在负载均衡器后面的服务器必须使用相同的私钥，或者周期性地在服务器间同步使用的私钥。\n证书废止  CRL(Certificate Revocation List)  CA厂商维护一个废止列表，浏览器请求获取此列表，缓存此列表，在认证时查询证书是否已作废。这种方式的问题有：客户端缓存无法及时失效；废止列表慢慢变大后的带宽消耗。\n OCSP(Online Certificate Status Protocol)  实时查询CA证书库，通过证书序列号获取证书状态。因此，CA厂商需要支持实时查询的负载，需要保证服务全球可用；进行实时查询时，认证过程被阻塞；存在泄漏客户访问地址到CA厂商的隐私风险。\n实际中，这两种废除证书的方式是共存的。\nTLS优化 计算负载 根据谷歌和facebook的实践经验，TLS计算占据不到1%的CPU负载，而且在现代硬件上，其计算速度足够快，不需要进行过多的优化。\n及早终止 在最坏的情况下，一次完整的TLS握手过程需要消耗3倍回环时间。因此，一个优化方式是及早终止TLS握手过程：在靠近客户端的区域部署本地服务器。这些本地服务器可以与实际执行业务的服务器建立长连接会话池，并将发送到本地服务器的请求代理到原始服务器上，这样来提高应用性能。\n    TLS Session重用 在使用Session Cache作为Session重用的技术时，需要注意如下几点：\n 多进程服务应共享session session cache的大小应与应用访问量匹配 session超时时间需进行配置 多服务器环境中，路由同一个IP或者同一个TLS Session Id到同一个服务器 在负载均衡环境中，使用外部共享缓存存储session 检查缓存命中统计数据  在使用Session ticket作用Session重用技术时，由于不是所有的客户端都支持此技术，因此，通常需要同时实现基于cache和ticket两种方案，将cache方案作为fallback\nTLS数据大小 每个TLS数据包中有20-40 byte用来存放header，MAC和其它内容。如果数据包的体积过大，那么这个数据包可能会超过MTU(典型值为1500 byte)，这是TLS数据包会被分成若干个packet在TCP/IP协议层传递，就有可能会因HOL问题产生较大的延时。如果数据包过小，其header导致的性能损耗比例相对提升。一种优化TLS数据包大小的方法是：在单个TLS包不超过MTU的前提下，包中包含的数据要尽可能多。\n例如，在MTU为1500 byte的环境中，分配IP层header为20 byte (40 byte for IPV6)，分配TCP framing 20 byte，分配TCP可选项40 byte，这样 TLS包的大小应为1420 byte。\nTLS压缩 TLS虽然支持压缩，但由于以下原因，不应再实际中使用\n “CRIME”攻击 TLS压缩不区分内容，会对已经压缩过得内容再次进行压缩，浪费CPU资源。  内容的压缩不应依赖于TLS压缩，相反，应在web服务器配置文本资源的GZip压缩，而音频/视频/图像应各自采用优化的压缩算法\n证书链的长度 浏览器认证时，从网站的证书开始向父节点遍历，直到根证书。因此，一个重要的优化是，在TLS握手返回结果中，包含所有需要认证的证书。虽然浏览器自身也可以获取这些中间证书，但这意味着认证暂停、DNS查询、TCP连接等待一系列耗时的行为\n另外，在证书链中排除不必要的证书。在理想条件下，发送到浏览器的证书应该只有两个：网站的证书和CA的证书。根证书是不必要的：如果浏览器中包含此根证书，那么网站是可信的；如果浏览器中不包含此根证书，那么及时发送根证书到浏览器，网站依然是不可信的。\nOCSP Stapling 前面提到浏览器端的OCSP会导致认证过程阻塞。OCSP Stapling允许服务端在发送证书链时，附带上OCSP response到浏览器。服务端可以缓存本服务证书OCSP response，避免客户端的多余请求。使用此方案时，需要考虑如下问题：\n OCSP response大小为400 byte - 4000 byte，可能会导致TLS数据包超过cwnd 证书链中只能包含一个OCSP response，如果证书链包含较多的证书，浏览器可能仍需要进行OCSP请求。  HSTS (HTTP Strict Transport Security) HSTS允许服务端通过声明一个header，强制要求浏览器在随后一段时间的仅能发送HTTPS请求到此域名及子域名。HSTS可以减少网络攻击，避免不必要的HTTP请求到HTTPS的重定向，来提高性能。\n1Strict-Transport-Security: max-age=31536000 其中，max-age单位为秒。\nChecklist  优化TCP性能 更新TLS库到最新 启用配置session cache和session ticket 监控session cache命中率 及早结束TLS握手 配置TLS大小到和MTU匹配 确保证书链大小不超过cwnd 移除不必要的证书，降低证书深度 禁用TLS压缩 配置SNI 配置OCSP 增加HSTP header  ","link":"https://ryji.github.io/post/hnbp/hpbn-tls/","section":"post","tags":["TLS","network"],"title":"High-Performance Browser Networking 读书笔记-TLS"},{"body":"","link":"https://ryji.github.io/tags/network/","section":"tags","tags":null,"title":"network"},{"body":"","link":"https://ryji.github.io/tags/tls/","section":"tags","tags":null,"title":"TLS"},{"body":"引言 UDP(User Datagram Protocol)协议，不同于TCP，是一种无连接的不可靠协议。在IP层的基础上，UDP协议增加了四个字段，且其中的port为可选的，checkSum也可以被业务层忽略。因此，UDP协议具有以下特点：\n 不保证消息送达（没有ACK，重传，timeout） 不保证消息顺序（没有packet编码，没有HOL问题） 无连接（没有连接建立/断开的状态） 无拥塞控制  另外，TCP为面向流的协议，因此，一条消息可能会被分片传输；而UDP协议不会有此情况，一条消息对应一个IP包，不会有分片的问题。\n    UDP与NAT 为了解决IPV4地址不足的问题，在1994年，人们提出使用NAT(Network Address Translator)设备将内网地址映射到外网地址，这样不同的子网就可以共享内网地址，来解决IPV4地址耗尽的问题。虽然这只是临时的解决方案，但如今NAT已经成为互联网网络基础设施的一部分。\n    NAT设备维护内部IP、端口与外部IP、端口的映射。TCP协议有明确的连接建立/断开流程，NAT设备可以利用在连接断开时，删除建立的映射关系；然而，UDP协议是无连接的，NAT设备必须维护映射表，处于内网的设备才能接受外部发过来的UDP消息。另外，NAT设备的映射表在没有流量经过一段时间后，会将映射关系置为timeout状态来删除这条映射关系记录。对于UDP应用来说，可以使用双向的keepAlive包来维持NAT设备的映射关系。 另外，虽然理论上NAT设备可以使用TCP连接的状态转移来维护映射关系，但实际上，大多数NAT设备都采用会话超时策略处理UDP和TCP连接。因此，当TCP连接失效时，可能是NAT设备的原因。\nSTUN, TURN and ICE UDP连接建立时，如果客户端设备隐藏在NAT设备之后，那么必须知道该设备的公网地址才能建立建立，而且，在NAT设备中，必须维护有此公网地址和设备内网地址的映射关系，进入此子网的流量才能经过NAT设备进入目标设备。STUN，TURN和ICE提供了建立端到端UDP连接的方法。\nSTUN STUN(Session Traversal Utilities for NAT)可以确认应用部署的环境是否存在NAT设备，如果存在NAT设备，则可以获取此连接的外网地址。STUN协议需要使用部署在外网环境的STUN服务器才能完成上述功能。\n     应用发出一条binding请求到STUN服务器，服务器返回应用的公网IP和端口 应用利用映射的公网IP和端口进行UDP通信 binding请求在请求的路由路径中建立了内外网地址映射关系，使用公网地址的入站流量可以正确地路由到内网地址 STUN协议也定义了一种keepalive的机制，用来维持NAT映射关系防止因超时导致的自动清理  TURN 在部分网络环境下，可能无法通过STUN建立UDP端到端连接。例如，部分企业级网络的防火墙可能禁用UDP协议。在此情况下，可以转用TURN(Traversal Using Relays around NAT)协议。顾名思义，TURN协议依赖公网上的转发来交换peer之间的数据\n     通信双方都发送allocate请求到同一个TURN服务器，进行通信协商 当协商完成后，通信双方发送数据到TURN服务器，服务器进行数据转发  google libjingle提供了STUN，TURN，ICE协商的能力，而且被使用在google talk等产品中。根据其文档，92%的P2P连接可以直接使用STUN建立，剩余的使用TURN建立\nICE ICE( Interactive Connectivity Establishment)提供了建立高效的P2P通信的方法。优先直接连接，其次考虑使用STUN，在其他方式建立失败后，使用TURN\n    Checklist  必须适应多种网络环境 应当控制数据传输速率 应当进行拥塞控制 应当与采用与TCP类似的方式使用带宽 应当能够在丢失数据后进行重传 不应当发送大于MTU的数据包 应当处理数据包丢失、重复、乱序 应当能够在2min的延时下保持鲁棒性 应当支持IPV4 UDPcheckSum特性，必须支持IPV6 checkSum 在需要时，可以发送keepalive包(最小间隔不高于15s)  ","link":"https://ryji.github.io/post/hnbp/hpbn-udp/","section":"post","tags":["UDP","network"],"title":"High-Performance Browser Networking 读书笔记-UDP"},{"body":"","link":"https://ryji.github.io/tags/udp/","section":"tags","tags":null,"title":"UDP"},{"body":"引言 TCP/IP网络是互联网的核心，IP(Internet Protocal)提供了host-host的路由和寻址协议，TCP(Transmission Control Protocal)协议在不可靠的通信管道上建立了可靠的网络传输，保障接收端接收到的数据和发送端发送的数据内容和顺序一致。虽然协议没有要求，但如今大部分HTTP的实现都建立在TCP协议的基础上。\nTCP协议概述 三次握手 TCP协议建立连接时需要进行三次握手，如下表所示\n   发送端  接收端     SYN (x=rand()) ------\u0026gt; -   - \u0026lt;------ SYN ACK (x+1 y=rand())   ACK (y+1 x+1) ------\u0026gt; -    发送端在发送ACK之后就可以向接收端发送业务数据，接收端需要在接收到发送端的ACK之后才能向发送端发送业务数据。如果端到端通信的延时较大(例如：物理距离较远)，那么建立起TCP连接的耗时较长。因此，基于TCP协议的一个常用的优化方式是重用已经建立的连接。\nTFO(TCP FAST OPEN, support on Linux 3.7+ kernel)技术可以在SYN packet中发送业务数据，降低部分因三次握手导致的延时。但这种技术只可以应用在部分类型的HTTP请求中，且SYN包中携带的数据包大小也有一定的限制。\n拥塞避免和控制 flow control flow control是一种用来避免发送过多的数据导致接收端过载的机制，为了达到这一目的，通信的双方需要在每个ACK包中添加自身的recieve window(rwnd)，使得发送可以根据接收方的rwnd来调整发送速度。当rwnd降为0时，发送端停止向接收端发送数据，直到接收端将自身buffer中的数据处理完毕\n最初版本的TCP协议为rwnd分配了16bit的空间，这样限定TCP通信过程中，双方的revieve buffer不能超过$ 2^{16} = 65536 $ bytes。随着应用对数据传输能力要求越来越高，RFC 1323提出 Window Scaling 技术，在ACK packet中将rwnd左移，recieve buffer提高到1G。\nslow-start flow-control解决了通信双方不会导致对方过载的问题，但实际上网络中的各个节点处理能力和带宽各有差异。在通信建立时，发送/接收双方无法了解参与通信各个节点的处理能力；随着通信的进行，网络环境发生变化，这时候也需要持续调整数据传输速率来使用网络状况。1988年，Van Jacobson 和 Michael J. Karels 提供这个问题的几个算法，包括：slow-start, congestion avoidance, fast retransmit 和 fast recovery。\n参数congestion window (cwnd)，这个参数限定了发送端在接收到ACK之前，可以发送数据的上限。因此，发送端未ACK的数据上限是$\\min(cwnd, rwnd)$。slow-start，顾名思义，在连接建立时，cwnd取较小的值，随着数据传输的进行，逐渐调整，直到达到网络允许的最优带宽，目前的调整方式为指数增长，线性降低，如下图所示。由于slow-start的限制，在连接建立的最初，应用程序无法使用全部的带宽。自Linux 2.6.39+ kernel开始，cwnd的初始默认值已经调整为10 segments     SSR(Slow-Start Restart)指当TCP连接处于idle状态一段时间后，为了避免因idle期间网络状况发生改变导致拥塞，其cwnd会重置为一个较小的值。对于使用 HTTP keepalive 连接的应用来说，这个参数也会影响其性能。\ncongestion avoidance 在实际的网络环境中，丢包(packet-loss)几乎是必然发生的。slow-start初始化一个较小的cwnd，然后随着每次ACK，将cwnd加倍，直到cwnd超过rwnd，或者cwnd超过系统上限，或者发生丢包。此时，congestion avoidance算法发挥作用，将cwnd适度减小，重复此过程，直到不再发生丢包。\n带宽-时延乘积(bandwidth-delay product) $$ 数据传输速率 * 回环时延 = min(cwnd, rwnd) $$\n如果窗口size过小，即使可用带宽很大，但大多数时间中，发送/接收端是在等待ACK消息，系统实际的发送/接收速率并不高。如果目标传输速率为10Mbps, 回环时间为100ms，对应的最小窗口大小为$ (10 * 1000000)bps * 0.1s/(8 * 1024) = 122.1KB $。不启用 window scaling 时，默认的 window size 为 64KB\nHOL(head of line) blocking 由于TCP的传输是有序且可信的，在传输过程中，如果有一个packet发生丢失，其后续所有的packet都需要等待丢失的packet重传到接收端后才能传输。这就是TCP的HOL问题。实际上，在TCP中，丢包是协议能达到最优性能所需反馈机制的重要组成部分。 如果应用能够容忍一部分丢包(音频、视频、游戏状态更新)，或者对时延比较敏感，可以改用UDP协议来传输数据，在应用层处理packet的乱序和丢失问题。WebRTC就选用UDP作为其基础协议。\nTCP协议优化  建立连接三次握手p的round-trip时延 新连接和长时间处于idle状态的连接的slow-start时延 拥塞控制和避免限制的吞吐率 cwnd大小正比于吞吐率  TCP 服务器  初始 Congestion Window Slow-Start Restart Window Scaling TCP Fast Open  应用  尽可能减少需要传输的数据 我们不能让数据传输得更快，但可以将数据部署得离用户更近 重用可用的TCP连接时优化应用性能的重要手段  Checklist  升级内核版本 (Linux: 3.2+) cwnd初始大小设置为10 禁用 slow-start after idle 打开 that window scaling 避免多余的数据传输 对需要传输的数据进行压缩 将服务区放置得靠近用于(CDN) 重用可用的TCP连接  ","link":"https://ryji.github.io/post/hnbp/hpbn-tcp/","section":"post","tags":["TCP","network"],"title":"High-Performance Browser Networking 读书笔记-TCP"},{"body":"","link":"https://ryji.github.io/tags/tcp/","section":"tags","tags":null,"title":"TCP"},{"body":"引言 GroupCache 是 go 语言的一个开源项目，其目标提供去中心节点的 P2P 分布式缓存代替 memcached。其中使用到的技术有：LRU cache，一致性哈希，并发请求压缩等。该项目已在 Google 的多个服务中使用。\n本文使用 C# 实现 GroupCache 中用到并发请求压缩技术，并使用单元测试验证其功能。\n实现 实现要点  需要记录所有执行中的请求，才能在新的请求进来时进行请求压缩，使用字典_onFlight记录；字典_onFlight存在多线程读写问题，需要使用锁来保护； 需要记录请求产生的结果和异常，才能在被阻塞的线程恢复后，将其他线程产生的结果返回到调用方。因此，在OnFlightCallRes添加Res和Exception字段 为了实现请求压缩，需要阻塞/恢复请求，且阻塞/恢复的单位是每一个同key的请求。使用ManualResetEventSlim进行线程的阻塞/恢复。因为阻塞/恢复的单元和执行结果都以key作为主键，将此变量放入OnFlightCallRes中  代码 1public class SingleFlight 2{ 3/// \u0026lt;summary\u0026gt; 4/// Keep Key After Execute Call 5/// \u0026lt;/summary\u0026gt; 6private readonly bool _keepKey; 78/// \u0026lt;summary\u0026gt; 9/// dic to store call result 10/// \u0026lt;/summary\u0026gt; 11private readonly Dictionary\u0026lt;string, OnFlightCallRes\u0026gt; _onFlight; 1213/// \u0026lt;summary\u0026gt; 14/// lock 15/// \u0026lt;/summary\u0026gt; 16private readonly ReaderWriterLockSlim _lockSlim; 1718public SingleFlight(bool keepKey) 19{ 20_keepKey = keepKey; 21_onFlight = new Dictionary\u0026lt;string, OnFlightCallRes\u0026gt;(); 22_lockSlim = new ReaderWriterLockSlim(); 23} 2425/// \u0026lt;summary\u0026gt; 26/// Return \u0026lt;paramref name=\u0026#34;callFunc\u0026#34;/\u0026gt; Result 27/// calls from different thread with the same \u0026lt;paramref name=\u0026#34;key\u0026#34;/\u0026gt; will be compressed to one call 28/// \u0026lt;/summary\u0026gt; 29/// \u0026lt;typeparam name=\u0026#34;T\u0026#34;\u0026gt;\u0026lt;/typeparam\u0026gt; 30/// \u0026lt;param name=\u0026#34;key\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; 31/// \u0026lt;param name=\u0026#34;callFunc\u0026#34;\u0026gt;\u0026lt;/param\u0026gt; 32/// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt; 33public T Call\u0026lt;T\u0026gt;(string key, Func\u0026lt;T\u0026gt; callFunc) 34{ 35_lockSlim.EnterWriteLock(); 36if (_onFlight.ContainsKey(key)) 37{ 38var callRes = _onFlight[key]; 39_lockSlim.ExitWriteLock(); 4041// wait on first call to finish 42callRes.ResetEventSlim.Wait(); 43return ResolveCallRes\u0026lt;T\u0026gt;(callRes); 44} 45else 46{ 47var callRes = new OnFlightCallRes() 48{ 49ResetEventSlim = new ManualResetEventSlim(), 50}; 51_onFlight.Add(key, callRes); 52_lockSlim.ExitWriteLock(); 53try 54{ 55callRes.Res = callFunc.Invoke(); 56} 57catch(Exception ex) 58{ 59callRes.Exception = ex; 60} 61// stop all waiting  62callRes.ResetEventSlim.Set(); 6364_lockSlim.EnterWriteLock(); 65if (!_keepKey) 66{ 67_onFlight.Remove(key); 68} 69_lockSlim.ExitWriteLock(); 70return ResolveCallRes\u0026lt;T\u0026gt;(callRes); 71} 72} 7374private T ResolveCallRes\u0026lt;T\u0026gt;(OnFlightCallRes callRes) 75{ 76if (callRes.Exception != null) 77{ 78throw callRes.Exception; 79} 80return (T)callRes.Res; 81} 8283/// \u0026lt;summary\u0026gt; 84/// clear key if \u0026lt;code\u0026gt;_keepKey\u0026lt;/code\u0026gt; is true 85/// \u0026lt;/summary\u0026gt; 86public void ClearKeys() 87{ 88if (!_keepKey) return; 8990_lockSlim.EnterWriteLock(); 91_onFlight.Clear(); 92_lockSlim.ExitWriteLock(); 93} 94} 959697/// \u0026lt;summary\u0026gt; 98/// Call Result 99/// \u0026lt;/summary\u0026gt; 100public class OnFlightCallRes 101{ 102/// \u0026lt;summary\u0026gt; 103/// Exception When Execute Call 104/// \u0026lt;/summary\u0026gt; 105public Exception Exception { get; set; } 106107/// \u0026lt;summary\u0026gt; 108/// ResetEvent 109/// \u0026lt;/summary\u0026gt; 110public ManualResetEventSlim ResetEventSlim { get; set; } 111112/// \u0026lt;summary\u0026gt; 113/// Result From Call 114/// \u0026lt;/summary\u0026gt; 115public Object Res { get; set; } 116117} 单元测试 测试要点 单元测试主要测试三个功能：\n 返回值是否符合预期TestReturnValue 异常是否正常抛出TestException 请求压缩  TestKeepKey测试保留key和清理key时的特性，三次请求的实际执行次数分别是 _upperLimit / _batchSize、0、_upperLimit / _batchSize TestNonKeepKey测试非保留key时的压缩特性，两次请求的实际执行次数均在区间 [ _upperLimit / _ batchSize, _upperLimit] DoMultiCall方法执行实际的请求，并返回请求次数和值。此处由于返回值数组res被多线程读写，需要加锁或者使用线程安全集合，这里选用线程安全集合ConcurrentBag来记录返回结果    代码 1public class SingleFlightTest 2{ 3private int _upperLimit = 50; 4private int _batchSize = 2; 56[TestMethod] 7public void TestReturnValue() 8{ 9var singleFlight = new SingleFlight(false); 10Assert.AreEqual(1, singleFlight.Call\u0026lt;int\u0026gt;(\u0026#34;1\u0026#34;, () =\u0026gt; 1)); 11} 1213[TestMethod] 14[ExpectedException(typeof(ArgumentException))] 15public void TestException() 16{ 17var singleFlight = new SingleFlight(false); 18singleFlight.Call\u0026lt;int\u0026gt;(\u0026#34;1\u0026#34;, () =\u0026gt; throw new ArgumentException()); 19} 2021[TestMethod] 22public void TestKeepKey() 23{ 24var keepKeyFlight = new SingleFlight(true); 25var multiCallRes = DoMultiCall(keepKeyFlight); 26for(int i = 0; i \u0026lt; _upperLimit; ++i) 27{ 28Assert.AreEqual(multiCallRes.Item1[i], i / _batchSize); 29} 30Assert.AreEqual(multiCallRes.Item2, _upperLimit/_batchSize); 3132// call cnt is zero when not clear keys 33multiCallRes = DoMultiCall(keepKeyFlight); 34Assert.AreEqual(multiCallRes.Item2, 0); 3536keepKeyFlight.ClearKeys(); 37multiCallRes = DoMultiCall(keepKeyFlight); 38Assert.AreEqual(multiCallRes.Item2, _upperLimit / _batchSize); 39} 4041[TestMethod] 42public void TestNonKeepKey() 43{ 44var keepKeyFlight = new SingleFlight(false); 4546var multiCallRes = DoMultiCall(keepKeyFlight); 4748for (int i = 0; i \u0026lt; _upperLimit; ++i) 49{ 50Assert.AreEqual(multiCallRes.Item1[i], i / _batchSize); 51} 5253// call cnt is in target range 54Assert.IsTrue(multiCallRes.Item2 \u0026gt;= _upperLimit / _batchSize); 55Assert.IsTrue(multiCallRes.Item2 \u0026lt;= _upperLimit); 5657// call cnt is in target range 58multiCallRes = DoMultiCall(keepKeyFlight); 59Assert.IsTrue(multiCallRes.Item2 \u0026gt;= _upperLimit/_batchSize); 60Assert.IsTrue(multiCallRes.Item2 \u0026lt;= _upperLimit); 61} 626364private Tuple\u0026lt;List\u0026lt;int\u0026gt;, int\u0026gt; DoMultiCall(SingleFlight singleFlight) 65{ 66var tList = new List\u0026lt;Task\u0026gt;(); 67// res is added in multiple thread, should use thread-safe collection 68var res = new ConcurrentBag\u0026lt;int\u0026gt;(); 69var callCnt = 0; 7071for (int i = 0; i \u0026lt; _upperLimit; ++i) 72{ 73var tmp = i; 74tList.Add(Task.Factory.StartNew( 75() =\u0026gt; { 76var callRes = singleFlight.Call\u0026lt;int\u0026gt;((tmp / _batchSize).ToString(), 77() =\u0026gt; { 78Thread.Sleep(50); 79Interlocked.Increment(ref callCnt); 80return tmp / _batchSize; 81}); 82res.Add(callRes); 83})); 84} 8586Task.WaitAll(tList.ToArray()); 87var list = res.ToList(); 88list.Sort(); 89return new Tuple\u0026lt;List\u0026lt;int\u0026gt;, int\u0026gt;(list, callCnt); 90} 91} ","link":"https://ryji.github.io/post/csharp/csharp_singleflight/","section":"post","tags":["csharp","go","groupcache"],"title":"C# version golang singleflight"},{"body":"","link":"https://ryji.github.io/tags/go/","section":"tags","tags":null,"title":"go"},{"body":"","link":"https://ryji.github.io/tags/groupcache/","section":"tags","tags":null,"title":"groupcache"},{"body":"","link":"https://ryji.github.io/categories/%E4%BB%A3%E7%A0%81/","section":"categories","tags":null,"title":"代码"},{"body":"","link":"https://ryji.github.io/categories/%E7%BC%93%E5%AD%98/","section":"categories","tags":null,"title":"缓存"},{"body":"引言 泛型（参数化类型），将程序逻辑从具体的类型中分离出来，从而提高代码的复用性。 .Net的System.Collections.Generic命名空间中定义的大多数类型使用了泛型技术，比如List\u0026lt;T\u0026gt;，Stack\u0026lt;T\u0026gt;，Tuple\u0026lt;T1, T2, T3, ...\u0026gt;，SortedList\u0026lt;TKey, TValue\u0026gt;等，另外，委托类型也提供了泛型委托Action和Function。泛型接口定义了一种参数化类型的接口，保障程序的类型安全。\n泛型树节点接口 以树的遍历为例，不管树节点的类型和实现是什么，只要知道当前节点的深度和其子节点，就可以进行树的遍历。因此，定义泛型树节点接口ITreeNode\u0026lt;T\u0026gt;。\n1public interface ITreeNode\u0026lt;T\u0026gt; where T : class 2{ 3int Depth { get; set; } //深度 4List\u0026lt;T\u0026gt; GetChildren();\t//获取子节点 5} 泛型树的约束 在泛型树的定义中添加约束class和ITreeNode\u0026lt;T\u0026gt;，这样，在编译期就可以确保类型安全，用未实现ITreeNode\u0026lt;T\u0026gt;接口的class来实例化Tree\u0026lt;T\u0026gt;时会编译出错。\n1public class Tree\u0026lt;T\u0026gt; where T : class, ITreeNode\u0026lt;T\u0026gt; 2{ 3//树根 4public T Root { get; set; } 5//最大深度 6public int MaxDepth { get; set; } 78public void Travel() 9{ 10Travel(Root, MaxDepth); 11} 1213public void Travel(int maxDepth) 14{ 15Debug.Assert(maxDepth \u0026gt;= 0); 16Travel(Root, Math.Min(maxDepth, MaxDepth)); 17} 1819public void Travel(T parentNode, int maxDepth) 20{ 21Debug.Assert(maxDepth \u0026lt;= MaxDepth, \u0026#34;maxdepth cannot be larger than tree Maxdepth\u0026#34;); 22if (parentNode.Depth \u0026gt; maxDepth) return; 23Console.WriteLine(parentNode.ToString()); 24var children = parentNode.GetChildren(); 25if (children == null) return; 26foreach (var child in children) 27{ 28Travel(child, maxDepth); 29} 30} 31} TreeNode实现 MyTreeNode实现了接口，通过静态属性GetChildrenFunc，具体的GetChildren可以由用户实现。若用户未定义GetChildren，则程序抛出异常。GetChildrenFunc也是通过泛型来确保编译期的类型安全。\n1public class MyTreeNode : ITreeNode\u0026lt;MyTreeNode\u0026gt; 2{ 3public int Content { get; set; } 4public static Func\u0026lt;MyTreeNode, List\u0026lt;MyTreeNode\u0026gt;\u0026gt; GetChildrenFunc { get; set; } 56#region constructor 78public MyTreeNode(int content = 0, int depth = 1) 9{ 10Content = content; 11Depth = depth; 12} 1314#endregion 1516#region field in interface 1718public int Depth { get; set; } 1920public List\u0026lt;MyTreeNode\u0026gt; GetChildren() 21{ 22if (GetChildrenFunc != null) 23{ 24return GetChildrenFunc(this); 25} 26throw new NotImplementedException(\u0026#34;GetChildrenFunc\u0026#34;); 27} 2829#endregion 3031public override string ToString() 32{ 33return string.Format(\u0026#34;depth is {0}, content is {1}\u0026#34;, Depth.ToString(), Content.ToString()); 34} 35} 3637//未实现接口，测试编译期类型安全 38public class MyTreeNode2 39{ 4041} 测试代码 1public class TestTree 2{ 3public static void Test() 4{ 5//无GetChildrenFunc的实现时，运行时抛出NotImplementedException 6MyTreeNode.GetChildrenFunc = (node =\u0026gt; new List\u0026lt;MyTreeNode\u0026gt;() 7{ 8new MyTreeNode(node.Content*2, node.Depth + 1), 9new MyTreeNode(node.Content*2 + 1, node.Depth + 1) 10}); 111213var tree1 = new Tree\u0026lt;MyTreeNode\u0026gt;() { Root = new MyTreeNode(1), MaxDepth = 3 }; 14tree1.Travel(); 1516//未注释时，无法通过编译 17//var tree2 = new Tree\u0026lt;MyTreeNode2\u0026gt;(); 1819} 20} ","link":"https://ryji.github.io/post/csharp/template_in_csharp/","section":"post","tags":["csharp"],"title":"Template in C#"},{"body":"","link":"https://ryji.github.io/series/","section":"series","tags":null,"title":"Series"}]